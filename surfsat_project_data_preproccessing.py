# -*- coding: utf-8 -*-
"""SurfSat Project - Data Preproccessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWYPcsZ6o02HuoZvz_kdGWKDUVTthaR2
"""

!pip install pykml

## Optimal Params for pictures: zoom = 16 , size = "1280x1280"

"""## Imports and Intalization"""

import requests
# from pykml import parser
from lxml import etree
import os
from lxml.etree import ParseError
from geopy.geocoders import Nominatim
import pickle
import pandas as pd
import re
import zipfile

from google.colab import drive
drive.mount('/content/drive')

import zipfile
def unzip_images(zip_path, extract_to):
    # Check if the extraction directory exists, create if not
    os.makedirs(extract_to, exist_ok=True)

    # Unzip the archive
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"Images extracted to {extract_to}")

"""
## Parse KML File"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile create_dataset.py
# def get_map_snapshot(api_key, coordinates, zoom=16, size="1280x1280", map_type="satellite"):
#     """
#     Function to get a map snapshot from Google Maps Static API.
# 
#     :param api_key: Your Google Maps Static API key.
#     :param coordinates: A tuple or list of the latitude and longitude (e.g., (39.6026, -9.0708)).
#     :param zoom: Zoom level of the map. Default is 12.
#     :param size: Size of the map image. Default is 600x300.
#     :param map_type: Type of the map (roadmap, satellite, hybrid, terrain). Default is satellite.
#     :return: The response content (image bytes).
#     """
#     base_url = "https://maps.googleapis.com/maps/api/staticmap?"
# 
#     params = {
#         "center": f"{coordinates[0]},{coordinates[1]}",
#         "zoom": zoom,
#         "size": size,
#         "maptype": map_type,
#         "key": api_key
#     }
# 
#     response = requests.get(base_url, params=params)
# 
#     if response.status_code == 200:
#         return response.content
#     else:
#         raise Exception(f"Error fetching map: {response.status_code} - {response.text}")
# 
# def save_image(image_content, directory, filename):
#     """
#     Function to save the image content to a file.
# 
#     :param image_content: The image content (bytes).
#     :param directory: The directory to save the image.
#     :param filename: The filename to save the image.
#     """
#     os.makedirs(directory, exist_ok=True)
#     filepath = os.path.join(directory, filename)
#     with open(filepath, "wb") as file:
#         file.write(image_content)
#     print(f"Saved image to: {filepath}")
#     return os.path.basename(filepath)
# 
# def get_location_details(latitude, longitude):
#     geolocator = Nominatim(user_agent="myGeoAPI")
#     location = geolocator.reverse((latitude, longitude), exactly_one=True)
#     return location.address if location else "Location details not found"
# 
# def parse_kml(file_path):
#     """
#     Function to parse a KML file using the pykml parser and extract coordinates, folder names, and placemark names.
# 
#     :param file_path: Path to the KML file.
#     :return: List of tuples, each containing coordinates in decimal degrees, folder name, and placemark name.
#     """
#     if not os.path.exists(file_path):
#         raise FileNotFoundError(f"KML file not found: {file_path}")
# 
#     details_list = []
# 
#     try:
#         with open(file_path, 'r', encoding='utf-8') as file:
#             doc = parser.parse(file).getroot()
# 
#         # Iterate over all folders in the document
#         for folder in doc.Document.findall('.//{http://www.opengis.net/kml/2.2}Folder'):
#             folder_name = folder.name.text if hasattr(folder, 'name') else "Unnamed Folder"
# 
#             # Iterate over all placemarks in the folder
#             for placemark in folder.findall('.//{http://www.opengis.net/kml/2.2}Placemark'):
#                 placemark_name = placemark.name.text if hasattr(placemark, 'name') else "Unnamed Placemark"
# 
#                 # Iterate over all coordinates in the placemark
#                 for coords in placemark.findall('.//{http://www.opengis.net/kml/2.2}coordinates'):
#                     coord_text = coords.text.strip()
#                     coord_parts = coord_text.split(',')
#                     if len(coord_parts) >= 2:
#                         lon, lat = coord_parts[0], coord_parts[1]
#                         try:
#                             loc_details = get_location_details(float(lat), float(lon))
#                             details_list.append([(float(lat), float(lon)) , folder_name, placemark_name,loc_details])
#                         except ValueError:
#                             print(f"Skipping invalid coordinates: {coord_text}")
#             #     continue_user = input('Continue?')
#             #     if continue_user == 'No':
#             #       break
#             # break
#     except ParseError as e:
#         print(f"Error parsing the KML file: {e}")
# 
#     return details_list
# 
# def process_kml_coordinates(api_key, kml_file_path, save_directory):
#     """
#     Function to process a KML file, fetch map snapshots, and save them with running numbers.
# 
#     :param api_key: Your Google Maps Static API key.
#     :param kml_file_path: Path to the KML file.
#     :param save_directory: Directory to save the images.
#     """
#     details_list = parse_kml(kml_file_path)
# 
#     for i, coords in enumerate(details_list, start=1):
#         try:
#             image_content = get_map_snapshot(api_key, coords[0])
#             save_image(image_content, save_directory, f"map_snapshot_{i}.png")
#         except Exception as e:
#             print(f"Error with coordinate {coords}: {e}")
# 
# def extract_country(location):
#     search_res = re.search(r'[\w\s]+$', location)
#     if search_res:
#       if search_res.group().strip() == 'Maroc ⵍⵎⵖⵔⵉⴱ المغرب':
#         country = 'Marraco'
#         return country
#       elif search_res.group().strip() == 'ישראל':
#         country = 'Israel'
#         return country
#       else:
#         return search_res.group().strip()
#     else:
#       return None  # If no country is found, return None
# 
# def add_terrain(country):
#   terrain_dict = {'Sri Lanka':'Tropical', 'Maldives': 'Tropical' ,\
#                   'El Salvador': 'Tropical', 'Costa Rica': 'Tropical',\
#                   'Ecuador': 'Tropical','Marraco': 'Desert','Panamá':'Tropical',\
#                   'Indonesia':'Tropical','Nicaragua':'Tropical'}
#   return terrain_dict.get(country, 'Unknown')
# 
# 
# def add_image_path(details_df):
#   api_key = 'AIzaSyAm0oqew7Qt1w6ARxk5aDiK0WugBV1Aigs'
#   for index, row in details_df.iterrows():
#     placemark = row['placemark_num']
#     coordinates = row['coordinates']
#     image_content = get_map_snapshot(api_key, coordinates)
#     image_file_path = save_image(image_content, save_directory, f"map_snapshot_{placemark}.png")
#     details_df.at[index, 'image_path'] = image_file_path  # Ensure the path is correctly assigned back to the DataFrame
#   return details_df
# 
# # if __name__ == "__main__":
# #     # Absolute path to the KML file
# #     kml_file_path = r"/content/Satalite Surfing Images.kml"
# 
# #     # Absolute path to the directory where you want to save the images
# #     save_directory = r"/content/Sat_Images"
# 
# #     # Your Google Maps Static API key
# #     api_key = "YOUR_API_KEY_HERE"
#       # data_without_photos = parse_kml(kml_file_path)
#       # details_df = pd.DataFrame(data_without_photos, columns=['coordinates', 'tagger', 'placemark_num', 'location'])
#       # details_df['country'] = details_df['location'].apply(extract_country)
#       # details_df['terrain'] = details_df['country'].apply(add_terrain)
#       # details_df_with_images = add_image_path(details_df)
#

"""## Feature Engineering

### Functions
"""

def extract_country(location):
    search_res = re.search(r'[\w\s]+$', location)
    if search_res:
      if search_res.group().strip() == 'Maroc ⵍⵎⵖⵔⵉⴱ المغرب':
        country = 'Marraco'
        return country
      elif search_res.group().strip() == 'ישראל':
        country = 'Israel'
        return country
      else:
        return search_res.group().strip()
    else:
      return None  # If no country is found, return None

def add_terrain(country):
  terrain_dict = {'Sri Lanka':'Tropical', 'Maldives': 'Tropical' ,\
                  'El Salvador': 'Tropical', 'Costa Rica': 'Tropical',\
                  'Ecuador': 'Tropical','Marraco': 'Desert','Panamá':'Tropical',\
                  'Indonesia':'Tropical','Nicaragua':'Tropical'}
  return terrain_dict.get(country, 'Unknown')


def add_image_path(details_df):
  api_key = 'AIzaSyAm0oqew7Qt1w6ARxk5aDiK0WugBV1Aigs'
  for index, row in details_df.iterrows():
    placemark = row['placemark_num']
    coordinates = row['coordinates']
    image_content = get_map_snapshot(api_key, coordinates)
    image_file_path = save_image(image_content, save_directory, f"map_snapshot_{placemark}.png")
    details_df.at[index, 'image_path'] = image_file_path  # Ensure the path is correctly assigned back to the DataFrame
  return details_df

"""## Creating the dataset

### Main
"""

# data_without_photos = parse_kml(kml_file_path)

# details_df = pd.DataFrame(data_without_photos, columns=['coordinates', 'tagger', 'placemark_num', 'location'])

# details_df['country'] = details_df['location'].apply(extract_country)
# details_df['terrain'] = details_df['country'].apply(add_terrain)
# details_df_with_images = add_image_path(details_df)

"""##Other Shit"""

def extract_country(row):
  contries_dict = {'Maroc':'Marraco','Nicaragua':'Nicaragua',}

EYAL_API_KEY = 'AIzaSyAm0oqew7Qt1w6ARxk5aDiK0WugBV1Aigs'
kml_file_path = r"/content/Satalite Surfing Images.kml"
save_directory = r"/content/Sat_Images"
# process_kml_coordinates(EYAL_API_KEY, kml_file_path, save_directory)

details_df = pd.DataFrame(columns=['cordinates','tagger','placemark_num','location'])

locs = [loc for loc in details_df['location']]

countries = [re.search(r'[\w\s]+$', location) for location in locs]

for loc in locs:
  search_res = re.search(r'[\w\s]+$', loc)
  if search_res:
    if search_res.group().strip() == 'Maroc ⵍⵎⵖⵔⵉⴱ المغرب':
      country = 'Marraco'
      print(country)
    elif search_res.group().strip() == 'ישראל':
      country = 'Israel'
      print(country)
    else:
      print(search_res.group().strip())

details_before_extraction = details_df.copy()

def extract_country(location):
    search_res = re.search(r'[\w\s]+$', location)
    if search_res:
      if search_res.group().strip() == 'Maroc ⵍⵎⵖⵔⵉⴱ المغرب':
        country = 'Marraco'
        return country
      elif search_res.group().strip() == 'ישראל':
        country = 'Israel'
        return country
      else:
        return search_res.group().strip()
    else:
      return None  # If no country is found, return None

# Apply the function to the 'location' column and create a new 'country' column
details_df['country'] = details_df['location'].apply(extract_country)

maldives_indices = [127, 128, 151]
details_df.loc[maldives_indices, 'country'] = "Maldives"
# Update all other rows where country is None to "Sri Lanka"
# Ensure we do not overwrite Maldives that we just updated
condition = details_df['country'].isnull() & (~details_df.index.isin(maldives_indices))
details_df.loc[condition, 'country'] = "Sri Lanka"

def add_terrain(country):
  terrain_dict = {'Sri Lanka':'Tropical', 'Maldives': 'Tropical' ,\
                  'El Salvador': 'Tropical', 'Costa Rica': 'Tropical',\
                  'Ecuador': 'Tropical','Marraco': 'Desert','Panamá':'Tropical',\
                  'Indonesia':'Tropical','Nicaragua':'Tropical'}
  return terrain_dict.get(country, 'Unknown')

details_df['terrain'] = details_df['country'].apply(add_terrain)

details_df

details_df

import zipfile
from google.colab import files

def zip_directory(folder_path, output_zip):
    """Zip the contents of an entire directory."""
    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                zipf.write(file_path, os.path.relpath(file_path, os.path.join(folder_path, '..')))

# Specify the directory you want to download
folder_path = '/content/Sat_Images'  # Adjust this to your folder path
output_zip = '/content/Sat_Images.zip'  # Name for the output ZIP file

# Create a ZIP file
zip_directory(folder_path, output_zip)

# Download the ZIP file
files.download(output_zip)

details_df_with_images.to_csv('/content/Satalite_Surf_image_data.csv')

with open('Satalite_Surf_image_data.pkl', 'wb') as file:
    pickle.dump(details_df_with_images, file)

"""maldives = index( 127,128,151)
sri lanka = index( 187)
"""

from geopy.geocoders import Nominatim

def get_location_details(latitude, longitude):
    geolocator = Nominatim(user_agent="myGeoAPI")
    location = geolocator.reverse((latitude, longitude), exactly_one=True)
    return location.address if location else "Location details not found"

# Example usage
print(get_location_details(30.54467815330961, -9.726458682969735))  # Coordinates for the Eiffel Tower

from geopy.geocoders import Nominatim

def get_location_details(latitude, longitude, user_agent):
    """
    Function to fetch location details using geopy with a specified user agent.

    :param latitude: Latitude of the location.
    :param longitude: Longitude of the location.
    :param user_agent: A user-specific agent string for the Nominatim service.
    :return: Address of the location or a message if not found.
    """
    geolocator = Nominatim(user_agent=user_agent)
    location = geolocator.reverse((latitude, longitude), exactly_one=True)
    return location.address if location else "Location details not found"

# Example usages with different user agents
print(get_location_details(30.54467815330961, -9.726458682969735, "myGeoApp_v1"))  # Example for Eiffel Tower
print(get_location_details(30.54467815330961, -9.726458682969735, "myProject_geo_service_2024"))
print(get_location_details(30.54467815330961, -9.726458682969735, "myGeoAPI"))
print(get_location_details(30.54467815330961, -9.726458682969735, "GetLoc"))
print(get_location_details(30.54467815330961, -9.726458682969735, "GetLoc"))

from geopy.geocoders import GoogleV3
def get_location_details_google(latitude, longitude, user_agent,APIKEY):
  geolocator = GoogleV3(api_key= "Code")
  ocation = geolocator.reverse((latitude, longitude), exactly_one=True)
  return location.address if location else "Location details not found"

"""# Data Orginazing"""

from google.colab import drive
drive.mount('/content/drive')

data_cleaned = pd.read_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_Image_cleaned_dataset.csv')

"""##Creating cleaned_df"""

full_data_df = pd.read_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_image_data.csv')
full_data_df = full_data_df.drop(columns=['Unnamed: 0','Unnamed: 10'])

# Splitting and flattening the 'extra_features' into a list of unique features
full_data_df['extra_features_list'] = full_data_df['extra features'].dropna().apply(lambda x: x.split(' + '))

# Flatten the list and get unique features
all_features = set()
full_data_df['extra_features_list'].dropna().apply(lambda features: [all_features.add(feature) for feature in features])

# Create columns for each feature and fill with binary values (0 or 1)
for feature in all_features:
    full_data_df[feature] = full_data_df['extra features'].apply(lambda x: 1 if isinstance(x, str) and feature in x else 0)

print(full_data_df)

full_data_df = full_data_df.drop(columns=['extra features'])

full_data_df.to_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_Image_cleaned_dataset.csv')

"""## Get feature dfs"""

def get_df_by_feature(df):
    # Positive examples
    bay_df = df[df['Bay'] == 1]
    rivermouth_df = df[df['Rivermouth'] == 1]
    wavepattern_df = df[df['Wave Pattern'] == 1]

    # Determine the number of positive examples
    num_bay = len(bay_df)
    num_rivermouth = len(rivermouth_df)
    num_wavepattern = len(wavepattern_df)

    # Negative examples, matched in number to positive examples
    non_bay_df = df[df['Bay'] == 0].sample(min(num_bay, len(df[df['Bay'] == 0])), random_state=42)
    non_rivermouth_df = df[df['Rivermouth'] == 0].sample(min(num_rivermouth, len(df[df['Rivermouth'] == 0])), random_state=42)
    non_wavepattern_df = df[df['Wave Pattern'] == 0].sample(min(num_wavepattern, len(df[df['Wave Pattern'] == 0])), random_state=42)

    # Append negative examples to each DataFrame
    bay_df = pd.concat([bay_df, non_bay_df]).sample(frac=1).reset_index(drop=True)  # Shuffle
    rivermouth_df = pd.concat([rivermouth_df, non_rivermouth_df]).sample(frac=1).reset_index(drop=True)  # Shuffle
    wavepattern_df = pd.concat([wavepattern_df, non_wavepattern_df]).sample(frac=1).reset_index(drop=True)  # Shuffle

    # DataFrame containing examples that do not belong to any specific class
    other_df = df[(df['Wave Pattern'] != 1) & (df['Rivermouth'] != 1) & (df['Bay'] != 1)]

    return bay_df, rivermouth_df, wavepattern_df, other_df

def get_balanced_onlygood_df(df):
    # Define positive examples: having any of the features
    onlygood_df = df[(df['Wave Pattern'] == 1) | (df['Rivermouth'] == 1) | (df['Bay'] == 1)]

    # Define negative examples: having none of the features
    non_onlygood_df = df[(df['Wave Pattern'] != 1) & (df['Rivermouth'] != 1) & (df['Bay'] != 1)]

    # Calculate the minimum number of examples to make the dataset balanced
    min_count = min(len(onlygood_df), len(non_onlygood_df))

    # Sample from both positive and negative examples to make them balanced
    onlygood_df_balanced = onlygood_df.sample(min_count, random_state=42)
    non_onlygood_df_balanced = non_onlygood_df.sample(min_count, random_state=42)

    # Combine and shuffle the balanced dataset
    balanced_df = pd.concat([onlygood_df_balanced, non_onlygood_df_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)

    return balanced_df

# Example usage
# Assuming 'df' is your DataFrame loaded with all the necessary columns
# balanced_onlygood_df = get_balanced_onlygood_df(data_cleaned)
# print(balanced_onlygood_df)

balanced_df = get_balanced_onlygood_df(data_cleaned)

balanced_df['Good'] = ((balanced_df['Wave Pattern'] == 1) | (balanced_df['Rivermouth'] == 1) | (balanced_df['Bay'] == 1)).astype(int)

def count_and_sum_features(df):
    # Count the occurrences of each feature and the absence of all
    is_bay = (df['Bay'] == 1)
    is_rivermouth = (df['Rivermouth'] == 1)
    is_wavepattern = (df['Wave Pattern'] == 1)
    is_none = (~is_bay & ~is_rivermouth & ~is_wavepattern)  # None of the features are present

    # Summing the True values to get the count of rows meeting the conditions
    total_bay = is_bay.sum()
    total_rivermouth = is_rivermouth.sum()
    total_wavepattern = is_wavepattern.sum()
    total_none = is_none.sum()

    # Sum of instances where any of Bay, Rivermouth, or Wave Pattern is present
    total_feature_present = total_bay + total_rivermouth + total_wavepattern

    print(f"Total 'Bay' instances: {total_bay}")
    print(f"Total 'Rivermouth' instances: {total_rivermouth}")
    print(f"Total 'Wave Pattern' instances: {total_wavepattern}")
    print(f"Total 'None' instances: {total_none}")
    print(f"Total instances with any feature (Bay, Rivermouth, Wave Pattern): {total_feature_present}")
    print(f"Total instances with none of the features: {total_none}")

# Seeing how our features of data look like
# count_and_sum_features(balanced_df)

bay_df,rivermouth_df,wavepattern_df,other_df = get_df_by_feature(data_cleaned)

len(other_df)

print(bay_df['Bay'].value_counts())
print(rivermouth_df['Rivermouth'].value_counts())
print(wavepattern_df['Wave Pattern'].value_counts())

"""#Data Uploading"""

import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
import os

def split_and_save_data(df, output_dir, stratify_by_cols, target_label):
    # Extend the stratification column to include the target label
    df['stratify_col'] = df[stratify_by_cols[0]] + "_" + df[stratify_by_cols[1]] + "_" + df[target_label].astype(str)
    counts = df['stratify_col'].value_counts()
    small_categories = counts[counts <5].index
    print(counts)
    # Assign a new category for small groups
    df['stratify_col_adjusted'] = df['stratify_col'].apply(lambda x: 'Other' if x in small_categories else x)
    adjusted_counts = df['stratify_col_adjusted'].value_counts()
    print(adjusted_counts)
    # Initialize the StratifiedShuffleSplit object
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)  # Split for test set 10%

    # Performing the first split to separate out the test set
    for train_val_index, test_index in sss.split(df, df['stratify_col_adjusted']):
        train_val_set = df.iloc[train_val_index]
        test_set = df.iloc[test_index]

    # Adjust test_size for validation set to be 20% of the remaining data
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  # Split for validation set

    for train_index, val_index in sss.split(train_val_set, train_val_set['stratify_col_adjusted']):
        train_set = train_val_set.iloc[train_index]
        val_set = train_val_set.iloc[val_index]

    # Save datasets based on target label
    def save_datasets(dataset, set_name):
        target_dir = os.path.join(output_dir, set_name)
        os.makedirs(target_dir, exist_ok=True)

        # Save the entire dataset
        dataset.to_csv(os.path.join(target_dir, f'{set_name}_set.csv'), index=False)

    # Perform saving for train, validation, and test sets
    save_datasets(train_set, 'train')
    save_datasets(val_set, 'val')
    save_datasets(test_set, 'test')

    print(f"Data split into train, validation, and test sets and saved to {output_dir}")

# Example usage
# Assuming wavepattern_df is your DataFrame, 'Bay' is your target_label, and you have 'terrain' and 'tagger' as stratify columns
# split_and_save_data(wavepattern_df, 'path/to/output', ['terrain', 'tagger'], 'Bay')

# split_and_save_data(wavepattern_df, '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model', ['terrain', 'tagger'],'Wave Pattern')
# split_and_save_data(rivermouth_df, '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model', ['terrain', 'tagger'],'Rivermouth')
# split_and_save_data(bay_df, '/content/drive/MyDrive/Satalite_Surf_Project/bay_model', ['terrain', 'tagger'],'Bay')
# split_and_save_data(other_df, '/content/drive/MyDrive/Satalite_Surf_Project/other_model', ['terrain', 'tagger'])
# split_and_save_data(other_df, '/content/drive/MyDrive/Satalite_Surf_Project/combined_model', ['terrain', 'tagger'])
# split_and_save_data(balanced_df, '/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model', ['terrain', 'tagger'],'Good')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile split_save_data.py
# import shutil
# 
# import pandas as pd
# from sklearn.model_selection import StratifiedShuffleSplit
# import os
# 
# def split_and_save_data(df, output_dir, stratify_by_cols, target_label):
#     # Extend the stratification column to include the target label
#     df['stratify_col'] = df[stratify_by_cols[0]] + "_" + df[stratify_by_cols[1]] + "_" + df[target_label].astype(str)
#     counts = df['stratify_col'].value_counts()
#     small_categories = counts[counts <5].index
#     print(counts)
#     # Assign a new category for small groups
#     df['stratify_col_adjusted'] = df['stratify_col'].apply(lambda x: 'Other' if x in small_categories else x)
#     adjusted_counts = df['stratify_col_adjusted'].value_counts()
#     print(adjusted_counts)
#     # Initialize the StratifiedShuffleSplit object
#     sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)  # Split for test set 10%
# 
#     # Performing the first split to separate out the test set
#     for train_val_index, test_index in sss.split(df, df['stratify_col_adjusted']):
#         train_val_set = df.iloc[train_val_index]
#         test_set = df.iloc[test_index]
# 
#     # Adjust test_size for validation set to be 20% of the remaining data
#     sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  # Split for validation set
# 
#     for train_index, val_index in sss.split(train_val_set, train_val_set['stratify_col_adjusted']):
#         train_set = train_val_set.iloc[train_index]
#         val_set = train_val_set.iloc[val_index]
# 
#     # Save datasets based on target label
#     def save_datasets(dataset, set_name):
#         target_dir = os.path.join(output_dir, set_name)
#         os.makedirs(target_dir, exist_ok=True)
# 
#         # Save the entire dataset
#         dataset.to_csv(os.path.join(target_dir, f'{set_name}_set.csv'), index=False)
# 
#     # Perform saving for train, validation, and test sets
#     save_datasets(train_set, 'train')
#     save_datasets(val_set, 'val')
#     save_datasets(test_set, 'test')
# 
#     print(f"Data split into train, validation, and test sets and saved to {output_dir}")
# def populate_images(csv_path, source_dir, target_dir, target_label):
#     # Ensure the base target directory exists
#     os.makedirs(target_dir, exist_ok=True)
# 
#     # Create subdirectories for target and non-target images
#     target_images_dir = os.path.join(target_dir, f'{target_label}')
#     non_target_images_dir = os.path.join(target_dir, f'not_{target_label}')
#     os.makedirs(target_images_dir, exist_ok=True)
#     os.makedirs(non_target_images_dir, exist_ok=True)
# 
#     # Read the CSV file
#     df = pd.read_csv(csv_path)
# 
#     # Copy each image listed in the CSV to the appropriate target directory
#     for index, row in df.iterrows():
#         image_name = row['image_path']
#         label = row[target_label]  # Assuming the CSV contains a column with boolean or 1/0 indicating target label
#         source_path = os.path.join(source_dir, image_name)
#         target_path = os.path.join(target_images_dir if label == 1 else non_target_images_dir, image_name)
# 
#         # Check if the source image exists before copying
#         if os.path.exists(source_path):
#             shutil.copy(source_path, target_path)
#         else:
#             print(f"Warning: {source_path} does not exist.")
# 
# def run_populate_images(model_name,target_label):
# # Directory paths
#   source_images_dir = '/content/images/Sat_Images'
#   train_dir = f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/train'
#   validation_dir = f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/val'
#   test_dir = f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/test'
# 
#   # Populate images for each set
#   populate_images(f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/train/train_set.csv', source_images_dir, train_dir,target_label)
#   populate_images(f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/val/val_set.csv', source_images_dir, validation_dir,target_label)
#   populate_images(f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/test/test_set.csv', source_images_dir, test_dir,target_label)
# 
# # Directory paths
# # source_images_dir = '/content/images/Sat_Images'
# # train_dir = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train/images'
# # validation_dir = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val/images'
# # test_dir = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/test/images'
# 
# # # Populate images for each set
# # populate_images('/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train/train_set.csv', source_images_dir, train_dir)
# # populate_images('/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val/val_set.csv', source_images_dir, validation_dir)
# # populate_images('/content/drive/MyDrive/Satalite_Surf_Project/bay_model/test/test_set.csv', source_images_dir, test_dir)
#

zip_path = '/content/drive/MyDrive/Satalite_Surf_Project/Sat_Images.zip'
unzipped_images_dir = '/content/images'
unzip_images(zip_path,unzipped_images_dir)

# run_populate_images('rivermouth_model','Rivermouth')
# run_populate_images('wavepattern_model','Wave Pattern')
# run_populate_images('bay_model','Bay')
# run_populate_images('alltogether_model','Good')

"""#Model Creation"""

# Set up CUDA in OS
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
# Import libabries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import seaborn as sn
import pandas as pd
import torchvision
from torchvision import *
from torch.utils.data import Dataset, DataLoader
from torchvision.io import read_image
import torchvision.transforms as T
from torchvision import datasets, models, transforms
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import matplotlib.pyplot as plt
import time
import copy

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
# Find out if a GPU is available
use_cuda = torch.cuda.is_available()
print(use_cuda)

"""## All together Model

## All Together Model
# Create transform function
transforms_train = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(), # data augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization
])
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
good_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/train'
good_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/val'
good_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/test'
# Apply for training and test data
train_dataset = datasets.ImageFolder(good_train_dir, transforms_train)
val_dataset = datasets.ImageFolder(good_val_dir, transforms_test)
test_dataset = datasets.ImageFolder(good_test_dir, transforms_test)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=0)
real_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)
print('Train dataset size:', len(train_dataset))
print('Validation dataset size:', len(val_dataset))
class_names = train_dataset.classes
print('Class names:', class_names)
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})
def imshow(input, title):
    # torch.Tensor => numpy
    input = input.numpy().transpose((1, 2, 0))
    # undo image normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    input = std * input + mean
    input = np.clip(input, 0, 1)
    # display images
    plt.imshow(input)
    plt.title(title)
    plt.show()
# load a batch of train image
iterator = iter(train_dataloader)
# visualize a batch of train image
inputs, classes = next(iterator)
out = torchvision.utils.make_grid(inputs[:4])
imshow(out, title=[class_names[x] for x in classes[:4]])
def get_labels(dataloader):
    labels = []
    for _, cls in dataloader:
        labels.extend(cls.numpy())
    return labels

train_labels = get_labels(train_dataloader)
val_labels = get_labels(val_dataloader)

from collections import Counter

train_label_counts = Counter(train_labels)
val_label_counts = Counter(val_labels)

import matplotlib.pyplot as plt

# Configure plot settings
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})

def plot_value_counts(train_counts, val_counts, class_names):
    # Prepare data
    classes = list(class_names.values())  # Assuming class_names is a dict mapping indices to class names
    train_values = [train_counts.get(i, 0) for i in range(len(classes))]
    val_values = [val_counts.get(i, 0) for i in range(len(classes))]

    # Creating subplots
    fig, ax = plt.subplots()
    index = range(len(classes))
    bar_width = 0.35

    train_bar = ax.bar(index, train_values, bar_width, label='Train')
    val_bar = ax.bar([p + bar_width for p in index], val_values, bar_width, label='Validation')

    ax.set_xlabel('Class')
    ax.set_ylabel('Counts')
    ax.set_title('Distribution of classes in Train and Validation Sets')
    ax.set_xticks([p + bar_width / 2 for p in index])
    ax.set_xticklabels(classes)
    ax.legend()

    plt.show()

# Assuming 'class_names' is a dict mapping label indices to names, like {0: 'Class0', 1: 'Class1', ...}
class_names = {0: 'Not Good', 1: 'Good'}
plot_value_counts(train_label_counts, val_label_counts, class_names)

# model30epoch = models.resnet50(pretrained=True)
modelgood = models.resnet50(pretrained=True)

num_features = modelgood.fc.in_features
print('Number of features from pre-trained model', num_features)

# Replace the fully-connected layer with a new one with a single output
# model.fc = nn.Linear(num_features, 1)

# Optionally, you can add a sigmoid activation right here in the model definition,
# but typically it's applied via the loss during training or post-processing during inference
modelgood.fc = nn.Linear(num_features, 2)


# If you have a specific device (e.g., GPU) set up

modelgood = modelgood.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(modelgood.parameters(), lr=0.0001, momentum=0.9)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
import time
import torch

def train_and_validate_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device):
    # Start time recording
    start_time = time.time()

    # Initialize lists for storing metrics
    train_loss = []
    train_accuracy = []
    val_loss = []
    val_accuracy = []

    # Loop for every epoch
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1} running")

        ''' Training Phase '''
        model.train()  # Set model to training mode
        running_loss = 0.0
        running_corrects = 0

        # Iterate over data.
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            # Backward + optimize
            loss.backward()
            optimizer.step()

            # Statistics
            running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_dataloader.dataset)
        epoch_acc = running_corrects.double() / len(train_dataloader.dataset) * 100

        # Store metrics
        train_loss.append(epoch_loss)
        train_accuracy.append(epoch_acc)

        # Print training progress
        print(f'[Train] Epoch: {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% Time: {time.time() - start_time:.4f}s')

        ''' Validation Phase '''
        model.eval()  # Set model to evaluate mode
        running_loss = 0.0
        running_corrects = 0

        with torch.no_grad():
            for inputs, labels in val_dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                # Statistics
                running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(val_dataloader.dataset)
            epoch_acc = running_corrects.double() / len(val_dataloader.dataset) * 100

            # Store metrics
            val_loss.append(epoch_loss)
            val_accuracy.append(epoch_acc)

            # Print validation progress
            print(f'[Validation] Epoch: {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% Time: {time.time() - start_time:.4f}s')

    return train_loss, train_accuracy, val_loss, val_accuracy



modelgood = modelgood.to(device)

train_loss, train_accuracy, val_loss, val_accuracy = train_and_validate_model(
    modelgood, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=70, device=device)

###Eval
num_epochs=70
import numpy as np
import matplotlib.pyplot as plt

# Function to calculate the moving average
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Assuming train_accuracy and val_accuracy are lists or numpy arrays
# Example: train_accuracy = np.array([...]), val_accuracy = np.array([...])

# Convert to numpy arrays if they are lists
train_accuracy_cpu = [x.item() for x in train_accuracy]
val_accuracy_cpu = [x.item() for x in val_accuracy]

# Apply moving average with a window size of 5 (you can adjust the window size)
window_size = 5
smoothed_train_accuracy = moving_average(train_accuracy_cpu, window_size)
smoothed_val_accuracy = moving_average(val_accuracy_cpu, window_size)

# Adjust the x-axis to match the length of the smoothed data
epochs = np.arange(1, num_epochs + 1)
smoothed_epochs = epochs[window_size - 1:]

# Plot
plt.figure(figsize=(8, 8))
plt.plot(smoothed_epochs, smoothed_train_accuracy, '-o')
plt.plot(smoothed_epochs, smoothed_val_accuracy, '-o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.title('Train vs Test Accuracy over time for All Together Model')
plt.show()



# Assuming you have a model loaded and set to evaluation mode
modelgood.eval()

# If you're using a GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
modelgood.to(device)

def imshow(inp, title=None):
    '''Imshow for Tensor.'''
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# Define the transformations
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Create the DataLoader
# test_dataset = datasets.ImageFolder(val_dataset, transform=transforms_test)
# real_test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

# Iterate over data.
for images, labels in val_dataloader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = modelgood(images)
    _, preds = torch.max(outputs, 1)

    # Convert image for display
    images = images.cpu().data
    for j in range(images.size()[0]):
        predicted_label = test_dataset.classes[preds[j]]
        actual_label = test_dataset.classes[labels[j]]
        plt.title(f'Predicted: {predicted_label} / Actual: {actual_label}')
        imshow(images[j])


from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Good for Surfing', 1: 'Good for Surfing'}
evaluate_model(model, val_dataloader, device, class_names)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/model_weights.pth'
torch.save(model.state_dict(), save_path)
"""

# Create transform function
transforms_train = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(), # data augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization
])
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

good_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/train'
good_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/val'
good_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/test'

# Apply for training and test data
train_dataset = datasets.ImageFolder(good_train_dir, transforms_train)
val_dataset = datasets.ImageFolder(good_val_dir, transforms_test)
test_dataset = datasets.ImageFolder(good_test_dir, transforms_test)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=0)
real_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)

print('Train dataset size:', len(train_dataset))
print('Validation dataset size:', len(val_dataset))
class_names = train_dataset.classes
print('Class names:', class_names)

plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})
def imshow(input, title):
    # torch.Tensor => numpy
    input = input.numpy().transpose((1, 2, 0))
    # undo image normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    input = std * input + mean
    input = np.clip(input, 0, 1)
    # display images
    plt.imshow(input)
    plt.title(title)
    plt.show()
# load a batch of train image
iterator = iter(train_dataloader)
# visualize a batch of train image
inputs, classes = next(iterator)
out = torchvision.utils.make_grid(inputs[:4])
imshow(out, title=[class_names[x] for x in classes[:4]])

def get_labels(dataloader):
    labels = []
    for _, cls in dataloader:
        labels.extend(cls.numpy())
    return labels

train_labels = get_labels(train_dataloader)
val_labels = get_labels(val_dataloader)

from collections import Counter

train_label_counts = Counter(train_labels)
val_label_counts = Counter(val_labels)

import matplotlib.pyplot as plt

# Configure plot settings
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})

def plot_value_counts(train_counts, val_counts, class_names):
    # Prepare data
    classes = list(class_names.values())  # Assuming class_names is a dict mapping indices to class names
    train_values = [train_counts.get(i, 0) for i in range(len(classes))]
    val_values = [val_counts.get(i, 0) for i in range(len(classes))]

    # Creating subplots
    fig, ax = plt.subplots()
    index = range(len(classes))
    bar_width = 0.35

    train_bar = ax.bar(index, train_values, bar_width, label='Train')
    val_bar = ax.bar([p + bar_width for p in index], val_values, bar_width, label='Validation')

    ax.set_xlabel('Class')
    ax.set_ylabel('Counts')
    ax.set_title('Distribution of classes in Train and Validation Sets')
    ax.set_xticks([p + bar_width / 2 for p in index])
    ax.set_xticklabels(classes)
    ax.legend()

    plt.show()

# Assuming 'class_names' is a dict mapping label indices to names, like {0: 'Class0', 1: 'Class1', ...}
class_names = {0: 'Not Good', 1: 'Good'}
plot_value_counts(train_label_counts, val_label_counts, class_names)

modelgood = models.resnet50(pretrained=True)

num_features = modelgood.fc.in_features
print('Number of features from pre-trained model', num_features)

modelgood.fc = nn.Linear(num_features, 2)

# If you have a specific device (e.g., GPU) set up

modelgood = modelgood.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(modelgood.parameters(), lr=0.0001, momentum=0.9)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

import time
import torch

def train_and_validate_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device):
    # Start time recording
    start_time = time.time()

    # Initialize lists for storing metrics
    train_loss = []
    train_accuracy = []
    val_loss = []
    val_accuracy = []

    # Loop for every epoch
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1} running")

        """ Training Phase """
        model.train()  # Set model to training mode
        running_loss = 0.0
        running_corrects = 0

        # Iterate over data.
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            # Backward + optimize
            loss.backward()
            optimizer.step()

            # Statistics
            running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_dataloader.dataset)
        epoch_acc = running_corrects.double() / len(train_dataloader.dataset) * 100

        # Store metrics
        train_loss.append(epoch_loss)
        train_accuracy.append(epoch_acc)

        # Print training progress
        print(f'[Train] Epoch: {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% Time: {time.time() - start_time:.4f}s')

        """ Validation Phase """
        model.eval()  # Set model to evaluate mode
        running_loss = 0.0
        running_corrects = 0

        with torch.no_grad():
            for inputs, labels in val_dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                # Statistics
                running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(val_dataloader.dataset)
            epoch_acc = running_corrects.double() / len(val_dataloader.dataset) * 100

            # Store metrics
            val_loss.append(epoch_loss)
            val_accuracy.append(epoch_acc)

            # Print validation progress
            print(f'[Validation] Epoch: {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% Time: {time.time() - start_time:.4f}s')

    return train_loss, train_accuracy, val_loss, val_accuracy

modelgood = modelgood.to(device)

train_loss, train_accuracy, val_loss, val_accuracy = train_and_validate_model(
    modelgood, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=70, device=device)

"""###Eval"""

num_epochs=70

import numpy as np
import matplotlib.pyplot as plt

# Function to calculate the moving average
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Assuming train_accuracy and val_accuracy are lists or numpy arrays
# Example: train_accuracy = np.array([...]), val_accuracy = np.array([...])

# Convert to numpy arrays if they are lists
train_accuracy_cpu = [x.item() for x in train_accuracy]
val_accuracy_cpu = [x.item() for x in val_accuracy]

# Apply moving average with a window size of 5 (you can adjust the window size)
window_size = 5
smoothed_train_accuracy = moving_average(train_accuracy_cpu, window_size)
smoothed_val_accuracy = moving_average(val_accuracy_cpu, window_size)

# Adjust the x-axis to match the length of the smoothed data
epochs = np.arange(1, num_epochs + 1)
smoothed_epochs = epochs[window_size - 1:]

# Plot
plt.figure(figsize=(8, 8))
plt.plot(smoothed_epochs, smoothed_train_accuracy, '-o')
plt.plot(smoothed_epochs, smoothed_val_accuracy, '-o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.title('Train vs Test Accuracy over time for All Together Model')
plt.show()

# Assuming you have a model loaded and set to evaluation mode
modelgood.eval()

# If you're using a GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
modelgood.to(device)

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# Define the transformations
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Create the DataLoader
# test_dataset = datasets.ImageFolder(val_dataset, transform=transforms_test)
# real_test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

# Iterate over data.
for images, labels in val_dataloader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = modelgood(images)
    _, preds = torch.max(outputs, 1)

    # Convert image for display
    images = images.cpu().data
    for j in range(images.size()[0]):
        predicted_label = test_dataset.classes[preds[j]]
        actual_label = test_dataset.classes[labels[j]]
        plt.title(f'Predicted: {predicted_label} / Actual: {actual_label}')
        imshow(images[j])

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Good for Surfing', 1: 'Good for Surfing'}
evaluate_model(model, val_dataloader, device, class_names)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/model_weights.pth'
torch.save(model.state_dict(), save_path)

"""## Rivermouth Model Creation"""

import torch
import torchvision.transforms as T
from PIL import Image
from IPython.display import display

# Load the image file
image_path = '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/train/Rivermouth/map_snapshot_Eyal_13.png'
image_tensor = torchvision.io.read_image(image_path)

# Confirm the tensor shape and type
print("Tensor shape:", image_tensor.shape)
print("Max pixel value:", image_tensor.max())

# Ensure the tensor is in the correct type and scale for display
if image_tensor.max() <= 1:
    image_tensor = image_tensor * 255  # Rescale to 0-255 if it was 0-1

image_tensor = image_tensor.to(torch.uint8)  # Ensure it's uint8 type

# Convert the tensor to a PIL Image for display
image_pil = T.ToPILImage()(image_tensor)

# Display the image
print("This is a Rivermouth Image")
display(image_pil)

"""### Preproccessing"""

# Create transform function
transforms_train = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(), # data augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization
])
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

rivermouth_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/train'
rivermouth_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/val'
rivermouth_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/test'

# Apply for training and test data
train_dataset = datasets.ImageFolder(rivermouth_train_dir, transforms_train)
val_dataset = datasets.ImageFolder(rivermouth_val_dir, transforms_test)
test_dataset = datasets.ImageFolder(rivermouth_test_dir, transforms_test)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=0)
real_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)

print('Train dataset size:', len(train_dataset))
print('Test dataset size:', len(val_dataset))
class_names = train_dataset.classes
print('Class names:', class_names)

plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})
def imshow(input, title):
    # torch.Tensor => numpy
    input = input.numpy().transpose((1, 2, 0))
    # undo image normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    input = std * input + mean
    input = np.clip(input, 0, 1)
    # display images
    plt.imshow(input)
    plt.title(title)
    plt.show()
# load a batch of train image
iterator = iter(train_dataloader)
# visualize a batch of train image
inputs, classes = next(iterator)
out = torchvision.utils.make_grid(inputs[:4])
imshow(out, title=[class_names[x] for x in classes[:4]])

def get_labels(dataloader):
    labels = []
    for _, cls in dataloader:
        labels.extend(cls.numpy())
    return labels

train_labels = get_labels(train_dataloader)
val_labels = get_labels(val_dataloader)

from collections import Counter

train_label_counts = Counter(train_labels)
val_label_counts = Counter(val_labels)

import matplotlib.pyplot as plt

# Configure plot settings
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})

def plot_value_counts(train_counts, val_counts, class_names):
    # Prepare data
    classes = list(class_names.values())  # Assuming class_names is a dict mapping indices to class names
    train_values = [train_counts.get(i, 0) for i in range(len(classes))]
    val_values = [val_counts.get(i, 0) for i in range(len(classes))]

    # Creating subplots
    fig, ax = plt.subplots()
    index = range(len(classes))
    bar_width = 0.35

    train_bar = ax.bar(index, train_values, bar_width, label='Train')
    val_bar = ax.bar([p + bar_width for p in index], val_values, bar_width, label='Validation')

    ax.set_xlabel('Class')
    ax.set_ylabel('Counts')
    ax.set_title('Distribution of classes in Train and Validation Sets')
    ax.set_xticks([p + bar_width / 2 for p in index])
    ax.set_xticklabels(classes)
    ax.legend()

    plt.show()

# Assuming 'class_names' is a dict mapping label indices to names, like {0: 'Class0', 1: 'Class1', ...}
class_names = {0: 'Not Rivermouth', 1: 'Rivermouth'}
plot_value_counts(train_label_counts, val_label_counts, class_names)

"""###Training"""

# model30epoch = models.resnet50(pretrained=True)
model70epoch = models.resnet50(pretrained=True)

model70epoch = models.resnet50(pretrained=True)
num_features = model70epoch.fc.in_features
print('Number of features from pre-trained model', num_features)
model70epoch.fc = nn.Linear(num_features, 2)
model70epoch = model70epoch.to(device)

# Replace the fully-connected layer with a new one with a single output
# model.fc = nn.Linear(num_features, 1)

# Optionally, you can add a sigmoid activation right here in the model definition,
# but typically it's applied via the loss during training or post-processing during inference
model70epoch.fc = nn.Linear(num_features, 2)


# If you have a specific device (e.g., GPU) set up

model70epoch = model70epoch.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model70epoch.parameters(), lr=0.0001, momentum=0.9)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

import time
import torch

def train_and_validate_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device):
    # Start time recording
    start_time = time.time()

    # Initialize lists for storing metrics
    train_loss = []
    train_accuracy = []
    val_loss = []
    val_accuracy = []

    # Loop for every epoch
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1} running")

        """ Training Phase """
        model.train()  # Set model to training mode
        running_loss = 0.0
        running_corrects = 0

        # Iterate over data.
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            # Backward + optimize
            loss.backward()
            optimizer.step()

            # Statistics
            running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_dataloader.dataset)
        epoch_acc = running_corrects.double() / len(train_dataloader.dataset) * 100

        # Store metrics
        train_loss.append(epoch_loss)
        train_accuracy.append(epoch_acc)

        # Print training progress
        print(f'[Train] Epoch: {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% Time: {time.time() - start_time:.4f}s')

        """ Validation Phase """
        model.eval()  # Set model to evaluate mode
        running_loss = 0.0
        running_corrects = 0

        with torch.no_grad():
            for inputs, labels in val_dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                # Statistics
                running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(val_dataloader.dataset)
            epoch_acc = running_corrects.double() / len(val_dataloader.dataset) * 100

            # Store metrics
            val_loss.append(epoch_loss)
            val_accuracy.append(epoch_acc)

            # Print validation progress
            print(f'[Validation] Epoch: {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}% Time: {time.time() - start_time:.4f}s')

    return train_loss, train_accuracy, val_loss, val_accuracy

model70epoch = model70epoch.to(device)

train_loss, train_accuracy, val_loss, val_accuracy = train_and_validate_model(
    model70epoch, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=70, device=device)

"""###Eval"""

num_epochs=70

import numpy as np
import matplotlib.pyplot as plt

# Function to calculate the moving average
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Assuming train_accuracy and val_accuracy are lists or numpy arrays
# Example: train_accuracy = np.array([...]), val_accuracy = np.array([...])

# Convert to numpy arrays if they are lists
train_accuracy_cpu = [x.item() for x in train_accuracy]
val_accuracy_cpu = [x.item() for x in val_accuracy]

# Apply moving average with a window size of 5 (you can adjust the window size)
window_size = 5
smoothed_train_accuracy = moving_average(train_accuracy_cpu, window_size)
smoothed_val_accuracy = moving_average(val_accuracy_cpu, window_size)

# Adjust the x-axis to match the length of the smoothed data
epochs = np.arange(1, num_epochs + 1)
smoothed_epochs = epochs[window_size - 1:]

# Plot
plt.figure(figsize=(12, 12))
plt.plot(smoothed_epochs, smoothed_train_accuracy, '-o')
plt.plot(smoothed_epochs, smoothed_val_accuracy, '-o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.title('Train vs Test Accuracy over time for Rivermouth')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Function to calculate the moving average
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Assuming train_accuracy and val_accuracy are lists or numpy arrays
# Example: train_accuracy = np.array([...]), val_accuracy = np.array([...])

# Convert to numpy arrays if they are lists
train_accuracy_cpu = [x.item() for x in train_accuracy]
val_accuracy_cpu = [x.item() for x in val_accuracy]

# Apply moving average with a window size of 5 (you can adjust the window size)
window_size = 5
smoothed_train_accuracy = moving_average(train_accuracy_cpu, window_size)
smoothed_val_accuracy = moving_average(val_accuracy_cpu, window_size)

# Adjust the x-axis to match the length of the smoothed data
epochs = np.arange(1, num_epochs + 1)
smoothed_epochs = epochs[window_size - 1:]

# Plot
plt.figure(figsize=(12, 12))
plt.plot(smoothed_epochs, smoothed_train_accuracy, '-o')
plt.plot(smoothed_epochs, smoothed_val_accuracy, '-o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.title(f'Train vs Test Accuracy over time for Wave Pattern with {num_epochs} Epochs')
plt.show()

# Assuming you have a model loaded and set to evaluation mode
model.eval()

# If you're using a GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# Define the transformations
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Create the DataLoader
test_dataset = datasets.ImageFolder(rivermouth_test_dir, transform=transforms_test)
real_test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

# Iterate over data.
for images, labels in real_test_dataloader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = model(images)
    _, preds = torch.max(outputs, 1)

    # Convert image for display
    images = images.cpu().data
    for j in range(images.size()[0]):
        imshow(images[j])
        predicted_label = test_dataset.classes[preds[j]]
        actual_label = test_dataset.classes[labels[j]]
        plt.title(f'Predicted: {predicted_label} / Actual: {actual_label}')

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Rivermouth', 1: 'Rivermouth'}
evaluate_model(model, val_dataloader, device, class_names)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Rivermouth', 1: 'Rivermouth'}
evaluate_model(model30epoch, val_dataloader, device, class_names)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Rivermouth', 1: 'Rivermouth'}
evaluate_model(model70epoch, val_dataloader, device, class_names)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Rivermouth', 1: 'Rivermouth'}
evaluate_model(model, val_dataloader, device, class_names)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/model_weights.pth'
torch.save(model.state_dict(), save_path)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/model_weights_nosigmoid.pth'
torch.save(model30epoch.state_dict(), save_path)

"""## Bay Model Creation

###Intaliztion
"""

# Set up CUDA in OS
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
# Import libabries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import seaborn as sn
import pandas as pd
import torchvision
from torchvision import *
from torch.utils.data import Dataset, DataLoader
from torchvision.io import read_image
import torchvision.transforms as T
from torchvision import datasets, models, transforms
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from PIL import Image
from IPython.display import display
import matplotlib.pyplot as plt
import time
import copy

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
# Find out if a GPU is available
use_cuda = torch.cuda.is_available()
print(use_cuda)

# Load the image file
image_path = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train/Bay/map_snapshot_Eyal_15_Good.png'
image_tensor = torchvision.io.read_image(image_path)

# Confirm the tensor shape and type
print("Tensor shape:", image_tensor.shape)
print("Max pixel value:", image_tensor.max())

# Ensure the tensor is in the correct type and scale for display
if image_tensor.max() <= 1:
    image_tensor = image_tensor * 255  # Rescale to 0-255 if it was 0-1

image_tensor = image_tensor.to(torch.uint8)  # Ensure it's uint8 type

# Convert the tensor to a PIL Image for display
image_pil = T.ToPILImage()(image_tensor)

# Display the image
print("This is a Bay Image")
display(image_pil)

# Create transform function
transforms_train = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(), # data augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization
])
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

bay_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train'
bay_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val'
bay_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/test'

# Apply for training and test data
train_dataset = datasets.ImageFolder(bay_train_dir, transforms_train)
val_dataset = datasets.ImageFolder(bay_val_dir, transforms_test)
test_dataset = datasets.ImageFolder(bay_test_dir, transforms_test)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=0)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

print('Train dataset size:', len(train_dataset))
print('Test dataset size:', len(val_dataset))
class_names = train_dataset.classes
print('Class names:', class_names)

plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})
def imshow(input, title):
    # torch.Tensor => numpy
    input = input.numpy().transpose((1, 2, 0))
    # undo image normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    input = std * input + mean
    input = np.clip(input, 0, 1)
    # display images
    plt.imshow(input)
    plt.title(title)
    plt.show()
# load a batch of train image
iterator = iter(train_dataloader)
# visualize a batch of train image
inputs, classes = next(iterator)
out = torchvision.utils.make_grid(inputs[:4])
imshow(out, title=[class_names[x] for x in classes[:4]])

def get_labels(dataloader):
    labels = []
    for _, cls in dataloader:
        labels.extend(cls.numpy())
    return labels

train_labels = get_labels(train_dataloader)
val_labels = get_labels(val_dataloader)

from collections import Counter

train_label_counts = Counter(train_labels)
val_label_counts = Counter(val_labels)

import matplotlib.pyplot as plt

# Configure plot settings
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})

def plot_value_counts(train_counts, val_counts, class_names):
    # Prepare data
    classes = list(class_names.values())  # Assuming class_names is a dict mapping indices to class names
    train_values = [train_counts.get(i, 0) for i in range(len(classes))]
    val_values = [val_counts.get(i, 0) for i in range(len(classes))]

    # Creating subplots
    fig, ax = plt.subplots()
    index = range(len(classes))
    bar_width = 0.35

    train_bar = ax.bar(index, train_values, bar_width, label='Train')
    val_bar = ax.bar([p + bar_width for p in index], val_values, bar_width, label='Validation')

    ax.set_xlabel('Class')
    ax.set_ylabel('Counts')
    ax.set_title('Distribution of classes in Train and Validation Sets')
    ax.set_xticks([p + bar_width / 2 for p in index])
    ax.set_xticklabels(classes)
    ax.legend()

    plt.show()

# Assuming 'class_names' is a dict mapping label indices to names, like {0: 'Class0', 1: 'Class1', ...}
class_names = {0: 'Not Bay', 1: 'Bay'}
plot_value_counts(train_label_counts, val_label_counts, class_names)

"""###Training"""

model = models.resnet50(pretrained=True)
model

num_features = model.fc.in_features
print('Number of features from pre-trained model', num_features)

model.fc = nn.Linear(num_features, 2)
# If you have a specific device (e.g., GPU) set up

model = model.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)
torch.manual_seed(42)
torch.cuda.manual_seed(42)

import time
import torch

num_epochs = 100   # Set number of epochs
start_time = time.time()  # Start time recording

# Initialize lists for storing metrics
train_loss = []
train_accuracy = []
val_loss = []
val_accuracy = []

for epoch in range(num_epochs):  # Loop for every epoch
    print("Epoch {} running".format(epoch))  # Printing message

    """ Training Phase """
    model.train()  # Set model to training mode
    running_loss = 0.0
    running_corrects = 0

    # Iterate over data.
    for inputs, labels in train_dataloader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

        # Backward + optimize
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(train_dataloader.dataset)  # Dividing by the total number of samples
    epoch_acc = running_corrects.double() / len(train_dataloader.dataset) * 100

    # Store metrics
    train_loss.append(epoch_loss)
    train_accuracy.append(epoch_acc)

    # Print training progress
    print('[Train] Epoch: {}/{} Loss: {:.4f} Acc: {:.2f}% Time: {:.4f}s'.format(
        epoch + 1, num_epochs, epoch_loss, epoch_acc, time.time() - start_time))

    """ Validation Phase """
    model.eval()  # Set model to evaluate mode
    running_loss = 0.0
    running_corrects = 0

    with torch.no_grad():
        for inputs, labels in val_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            # Statistics
            running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(val_dataloader.dataset)  # Dividing by the total number of samples
        epoch_acc = running_corrects.double() / len(val_dataloader.dataset) * 100

        # Store metrics
        val_loss.append(epoch_loss)
        val_accuracy.append(epoch_acc)

        # Print validation progress
        print('[Validation] Epoch: {}/{} Loss: {:.4f} Acc: {:.2f}% Time: {:.4f}s'.format(
            epoch + 1, num_epochs, epoch_loss, epoch_acc, time.time() - start_time))

"""###Eval"""

import numpy as np
import matplotlib.pyplot as plt

# Function to calculate the moving average
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Assuming train_accuracy and val_accuracy are lists or numpy arrays
# Example: train_accuracy = np.array([...]), val_accuracy = np.array([...])

# Convert to numpy arrays if they are lists
train_accuracy_cpu = [x.item() for x in train_accuracy]
val_accuracy_cpu = [x.item() for x in val_accuracy]

# Apply moving average with a window size of 5 (you can adjust the window size)
window_size = 5
smoothed_train_accuracy = moving_average(train_accuracy_cpu, window_size)
smoothed_val_accuracy = moving_average(val_accuracy_cpu, window_size)

# Adjust the x-axis to match the length of the smoothed data
epochs = np.arange(1, num_epochs + 1)
smoothed_epochs = epochs[window_size - 1:]

# Plot
plt.figure(figsize=(12, 12))
plt.plot(smoothed_epochs, smoothed_train_accuracy, '-o')
plt.plot(smoothed_epochs, smoothed_val_accuracy, '-o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.title('Train vs Test Accuracy over time for Bay')
plt.show()

model.eval()

# If you're using a GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# Define the transformations
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Create the DataLoader
test_dataset = datasets.ImageFolder(bay_test_dir, transform=transforms_test)
real_test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

# Iterate over data.
for images, labels in real_test_dataloader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = model(images)
    _, preds = torch.max(outputs, 1)

    # Convert image for display
    images = images.cpu().data
    for j in range(images.size()[0]):
        predicted_label = test_dataset.classes[preds[j]]
        actual_label = test_dataset.classes[labels[j]]
        plt.title(f'Predicted: {predicted_label} / Actual: {actual_label}')
        imshow(images[j])

import seaborn as sns
def evaluate_model_and_plot_confusion_matrix(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
evaluate_model_and_plot_confusion_matrix(model, test_dataloader, device, class_names)

import seaborn as sns
def evaluate_model_and_plot_confusion_matrix(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
evaluate_model_and_plot_confusion_matrix(model, test_dataloader, device, class_names)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/model_weights_nosigmoid.pth'
torch.save(model.state_dict(), save_path)

"""## Wave Pattern Model Creation

###Intalization
"""

# Set up CUDA in OS
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
# Import libabries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import seaborn as sn
import pandas as pd
import torchvision
from torchvision import *
from torch.utils.data import Dataset, DataLoader
from torchvision.io import read_image
import torchvision.transforms as T
from torchvision import datasets, models, transforms
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from PIL import Image
from IPython.display import display
import matplotlib.pyplot as plt
import time
import copy

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
# Setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
# Find out if a GPU is available
use_cuda = torch.cuda.is_available()
print(use_cuda)

import torch
import torchvision.transforms as T
from PIL import Image
from IPython.display import display

# Load the image file
image_path = '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/train/Wave Pattern/map_snapshot_Eyal 2.png'
image_tensor = torchvision.io.read_image(image_path)

# Confirm the tensor shape and type
print("Tensor shape:", image_tensor.shape)
print("Max pixel value:", image_tensor.max())

# Ensure the tensor is in the correct type and scale for display
if image_tensor.max() <= 1:
    image_tensor = image_tensor * 255  # Rescale to 0-255 if it was 0-1

image_tensor = image_tensor.to(torch.uint8)  # Ensure it's uint8 type

# Convert the tensor to a PIL Image for display
image_pil = T.ToPILImage()(image_tensor)

# Display the image
print("This is a Wave Pattern Image")
display(image_pil)

# Create transform function
transforms_train = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(), # data augmentation
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization
])
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

wavepattern_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/train'
wavepattern_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/val'
wavepattern_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/test'

# Apply for training and test data
train_dataset = datasets.ImageFolder(wavepattern_train_dir, transforms_train)
val_dataset = datasets.ImageFolder(wavepattern_val_dir, transforms_test)
test_dataset = datasets.ImageFolder(wavepattern_test_dir, transforms_test)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=0)
real_test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)

print('Train dataset size:', len(train_dataset))
print('Val dataset size:', len(val_dataset))
class_names = train_dataset.classes
print('Class names:', class_names)

plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})
def imshow(input, title):
    # torch.Tensor => numpy
    input = input.numpy().transpose((1, 2, 0))
    # undo image normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    input = std * input + mean
    input = np.clip(input, 0, 1)
    # display images
    plt.imshow(input)
    plt.title(title)
    plt.show()
# load a batch of train image
iterator = iter(train_dataloader)
# visualize a batch of train image
inputs, classes = next(iterator)
out = torchvision.utils.make_grid(inputs[:4])
imshow(out, title=[class_names[x] for x in classes[:4]])

def get_labels(dataloader):
    labels = []
    for _, cls in dataloader:
        labels.extend(cls.numpy())
    return labels

train_labels = get_labels(train_dataloader)
val_labels = get_labels(val_dataloader)

from collections import Counter

train_label_counts = Counter(train_labels)
val_label_counts = Counter(val_labels)

import matplotlib.pyplot as plt

# Configure plot settings
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 60
plt.rcParams.update({'font.size': 20})

def plot_value_counts(train_counts, val_counts, class_names):
    # Prepare data
    classes = list(class_names.values())  # Assuming class_names is a dict mapping indices to class names
    train_values = [train_counts.get(i, 0) for i in range(len(classes))]
    val_values = [val_counts.get(i, 0) for i in range(len(classes))]

    # Creating subplots
    fig, ax = plt.subplots()
    index = range(len(classes))
    bar_width = 0.35

    train_bar = ax.bar(index, train_values, bar_width, label='Train')
    val_bar = ax.bar([p + bar_width for p in index], val_values, bar_width, label='Validation')

    ax.set_xlabel('Class')
    ax.set_ylabel('Counts')
    ax.set_title('Distribution of classes in Train and Validation Sets')
    ax.set_xticks([p + bar_width / 2 for p in index])
    ax.set_xticklabels(classes)
    ax.legend()

    plt.show()

# Assuming 'class_names' is a dict mapping label indices to names, like {0: 'Class0', 1: 'Class1', ...}
class_names = {0: 'Not Wave Pattern', 1: 'Wave Pattern'}
plot_value_counts(train_label_counts, val_label_counts, class_names)

"""###Training"""

model = models.resnet50(pretrained=True)
num_features = model.fc.in_features
print('Number of features from pre-trained model', num_features)

model.fc = nn.Linear(num_features, 2)


# If you have a specific device (e.g., GPU) set up

model = model.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

import time
import torch

num_epochs = 70   # Set number of epochs
start_time = time.time()  # Start time recording

# Initialize lists for storing metrics
train_loss = []
train_accuracy = []
val_loss = []
val_accuracy = []

for epoch in range(num_epochs):  # Loop for every epoch
    print("Epoch {} running".format(epoch))  # Printing message

    """ Training Phase """
    model.train()  # Set model to training mode
    running_loss = 0.0
    running_corrects = 0

    # Iterate over data.
    for inputs, labels in train_dataloader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

        # Backward + optimize
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(train_dataloader.dataset)  # Dividing by the total number of samples
    epoch_acc = running_corrects.double() / len(train_dataloader.dataset) * 100

    # Store metrics
    train_loss.append(epoch_loss)
    train_accuracy.append(epoch_acc)

    # Print training progress
    print('[Train] Epoch: {}/{} Loss: {:.4f} Acc: {:.2f}% Time: {:.4f}s'.format(
        epoch + 1, num_epochs, epoch_loss, epoch_acc, time.time() - start_time))

    """ Validation Phase """
    model.eval()  # Set model to evaluate mode
    running_loss = 0.0
    running_corrects = 0

    with torch.no_grad():
        for inputs, labels in val_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            # Statistics
            running_loss += loss.item() * inputs.size(0)  # Multiply by batch size
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(val_dataloader.dataset)  # Dividing by the total number of samples
        epoch_acc = running_corrects.double() / len(val_dataloader.dataset) * 100

        # Store metrics
        val_loss.append(epoch_loss)
        val_accuracy.append(epoch_acc)

        # Print validation progress
        print('[Validation] Epoch: {}/{} Loss: {:.4f} Acc: {:.2f}% Time: {:.4f}s'.format(
            epoch + 1, num_epochs, epoch_loss, epoch_acc, time.time() - start_time))

"""### Eval"""

import numpy as np
import matplotlib.pyplot as plt

# Function to calculate the moving average
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# Assuming train_accuracy and val_accuracy are lists or numpy arrays
# Example: train_accuracy = np.array([...]), val_accuracy = np.array([...])

# Convert to numpy arrays if they are lists
train_accuracy_cpu = [x.item() for x in train_accuracy]
val_accuracy_cpu = [x.item() for x in val_accuracy]

# Apply moving average with a window size of 5 (you can adjust the window size)
window_size = 5
smoothed_train_accuracy = moving_average(train_accuracy_cpu, window_size)
smoothed_val_accuracy = moving_average(val_accuracy_cpu, window_size)

# Adjust the x-axis to match the length of the smoothed data
epochs = np.arange(1, num_epochs + 1)
smoothed_epochs = epochs[window_size - 1:]

# Plot
plt.figure(figsize=(8, 8))
plt.plot(smoothed_epochs, smoothed_train_accuracy, '-o')
plt.plot(smoothed_epochs, smoothed_val_accuracy, '-o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.title('Train vs Test Accuracy over time for Wave Pattern')
plt.show()

# Assuming you have a model loaded and set to evaluation mode
model.eval()

# If you're using a GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# Define the transformations
transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Create the DataLoader
test_dataset = datasets.ImageFolder(wavepattern_test_dir, transform=transforms_test)
real_test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

# Iterate over data.
for images, labels in real_test_dataloader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = model(images)
    _, preds = torch.max(outputs, 1)

    # Convert image for display
    images = images.cpu().data
    for j in range(images.size()[0]):
        predicted_label = test_dataset.classes[preds[j]]
        actual_label = test_dataset.classes[labels[j]]
        plt.title(f'Predicted: {predicted_label} / Actual: {actual_label}')
        imshow(images[j])

print(wavepattern_test_val_dataset.classes)
print(train_dataset.classes)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix for Wave Pattern Model')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Wave Pattern', 1: 'Not Wave Pattern'}
evaluate_model(model, wavepattern_val_test_dataloader, device, class_names)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            print(f'outputs are {outputs}')
            _, preds = torch.max(outputs, 1)
            print(f'preds are {preds}')

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
# class_names = {0: 'Not Wave Pattern', 1: 'Wave Pattern'}
# evaluate_model(wavepattern_model, real_test_dataloader, device, class_names)

wavepattern_train_dataset.classes

class_names = {0: 'Not Wave Pattern', 1: 'Wave Pattern'}
evaluate_model(wavepattern_model, wavepattern_real_test_dataloader, device, class_names)

class_names = {1: 'Not Wave Pattern', 0: 'Wave Pattern'}
evaluate_model(wavepattern_model, wavepattern_real_test_dataloader, device, class_names)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/model_weights.pth'
torch.save(model.state_dict(), save_path)

save_path = '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/model_weights_nosigmoid.pth'
torch.save(model.state_dict(), save_path)

"""#Combining the Models

###Loading the models
"""

import torch
import torchvision.models as models
import torch.nn as nn

# Assuming the model was a ResNet50
def initialize_model():
    model = models.resnet50(pretrained=False)  # pretrained=False as we are loading custom weights
    num_features = model.fc.in_features
    # Modify the fully connected layer based on how it was modified when training
    model.fc = nn.Linear(num_features, 2)  # Adjust according to your specific setup
    return model

def load_model_weights(model, model_path):
    model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))
    model.eval()  # Set the model to evaluation mode
    return model

wavepattern_model = initialize_model()
bay_model = initialize_model()
# rivermouth_model = initialize_model()

# Paths to the saved models
wavepattern_path = '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/model_weights_nosigmoid.pth'
bay_path = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/model_weights_nosigmoid.pth'
# rivermouth_path = '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/model_weights_nosigmoid.pth'

# Load the models
wavepattern_model = load_model_weights(wavepattern_model, wavepattern_path)
bay_model = load_model_weights(bay_model, bay_path)
# rivermouth_model = load_model_weights(rivermouth_model, rivermouth_path)

# Move Models to GPU

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

wavepattern_model = wavepattern_model.to(device)
bay_model = bay_model.to(device)
# rivermouth_model = rivermouth_model.to(device)

transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

rivermouth_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/val + test'
rivermouth_test_val_dataset = datasets.ImageFolder(rivermouth_test_val_dir, transforms_test)
rivermouth_val_test_dataloader = torch.utils.data.DataLoader(rivermouth_test_val_dataset, batch_size=12, shuffle=False, num_workers=0)



wavepattern_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/val + test'
wavepattern_test_val_dataset = datasets.ImageFolder(wavepattern_test_val_dir, transforms_test)
wavepattern_val_test_dataloader = torch.utils.data.DataLoader(wavepattern_test_val_dataset, batch_size=12, shuffle=False, num_workers=0)

transforms_train = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])


transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

bay_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train'
bay_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val'
bay_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/test'

rivermouth_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/train'
rivermouth_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/val'
rivermouth_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/test'
rivermouth_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/val + test'


wavepattern_train_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/train'
wavepattern_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/val'
wavepattern_test_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/test'

# Apply for training and test data
bay_train_dataset = datasets.ImageFolder(bay_train_dir, transforms_train)
bay_val_dataset = datasets.ImageFolder(bay_val_dir, transforms_test)
bay_test_dataset = datasets.ImageFolder(bay_test_dir, transforms_test)

rivermouth_train_dataset = datasets.ImageFolder(rivermouth_train_dir, transforms_train)
rivermouth_val_dataset = datasets.ImageFolder(rivermouth_val_dir, transforms_test)
rivermouth_test_dataset = datasets.ImageFolder(rivermouth_test_dir, transforms_test)

wavepattern_train_dataset = datasets.ImageFolder(wavepattern_train_dir, transforms_train)
wavepattern_val_dataset = datasets.ImageFolder(wavepattern_val_dir, transforms_test)
wavepattern_test_dataset = datasets.ImageFolder(wavepattern_test_dir, transforms_test)

bay_train_dataloader = torch.utils.data.DataLoader(bay_train_dataset, batch_size=12, shuffle=False, num_workers=0)
bay_val_dataloader = torch.utils.data.DataLoader(bay_val_dataset, batch_size=12, shuffle=False, num_workers=0)
bay_real_test_dataloader = torch.utils.data.DataLoader(bay_test_dataset, batch_size=12, shuffle=False, num_workers=0)

rivermouth_train_dataloader = torch.utils.data.DataLoader(rivermouth_train_dataset, batch_size=12, shuffle=False, num_workers=0)
rivermouth_val_dataloader = torch.utils.data.DataLoader(rivermouth_val_dataset, batch_size=12, shuffle=False, num_workers=0)
rivermouth_real_test_dataloader = torch.utils.data.DataLoader(rivermouth_test_dataset, batch_size=12, shuffle=False, num_workers=0)

wavepattern_train_dataloader = torch.utils.data.DataLoader(rivermouth_train_dataset, batch_size=12, shuffle=False, num_workers=0)
wavepattern_val_dataloader = torch.utils.data.DataLoader(wavepattern_val_dataset, batch_size=12, shuffle=False, num_workers=0)
wavepattern_real_test_dataloader = torch.utils.data.DataLoader(wavepattern_test_dataset, batch_size=12, shuffle=False, num_workers=0)

import torch
class FinalPredictor():
  def __init__(self, model_wave_weights_path, model_bay_weights_path, model_rivermouth_weights_path, device='cuda',used_sigmoid=False):
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Initializing models on {self.device}')

    # Initialize models and move them to the specified device
    self.model_wave = self.initialize_model().to(self.device)
    self.load_model_weights(self.model_wave, model_wave_weights_path)

    self.model_bay = self.initialize_model().to(self.device)
    self.load_model_weights(self.model_bay, model_bay_weights_path)

    self.model_rivermouth = self.initialize_model().to(self.device)
    self.load_model_weights(self.model_rivermouth, model_rivermouth_weights_path)

  # Assuming the model was a ResNet50
  def initialize_model(self):
      model = models.resnet50(pretrained=False)  # pretrained=False as we are loading custom weights
      num_features = model.fc.in_features
      # Modify the fully connected layer based on how it was modified when training
      model.fc = nn.Linear(num_features, 2)  # Adjust according to your specific setup
      return model

  def load_model_weights(self,model, model_path):
      model.load_state_dict(torch.load(model_path,map_location=self.device))
      model.eval()  # Set the model to evaluation mode
      return model

  def get_model_device(self,model):
    # Get the device of the first parameter of the model
    device = next(model.parameters()).device
    return device



  def predict_surfing(self, dataloader):
    # Ensure the device setup is properly referenced
    prediction_log = []  # List to store prediction details
    self.model_wave.eval()
    self.model_bay.eval()
    self.model_rivermouth.eval()

    # Print device information
    print(f"Wave model is on: {self.get_model_device(self.model_wave)}")
    print(f"Bay model is on: {self.get_model_device(self.model_bay)}")
    print(f"Rivermouth model is on: {self.get_model_device(self.model_rivermouth)}")

    with torch.no_grad():  # No gradient needed for prediction step
        for i, (inputs, labels) in enumerate(dataloader):
            inputs = inputs.to(self.device)  # Move inputs to the device the model is on
            labels = labels.to(self.device)

            # Process entire batch through each model
            output_wave = self.model_wave(inputs)
            _, preds_wave = torch.max(output_wave, 1)

            output_bay = self.model_bay(inputs)
            _, preds_bay = torch.max(output_bay, 1)

            output_rivermouth = self.model_rivermouth(inputs)
            _, preds_rivermouth = torch.max(output_rivermouth, 1)

            overall_predictions = (preds_wave == 1) | (preds_bay == 1) | (preds_rivermouth == 1)

            # Create log entry for the batch
            batch_details = {
                'batch_index': i,
                'labels': labels.cpu().tolist(),  # Convert labels to list and store
                'wave_predictions': preds_wave.cpu().tolist(),
                'bay_predictions': preds_bay.cpu().tolist(),
                'rivermouth_predictions': preds_rivermouth.cpu().tolist(),
                'overall predictions': overall_predictions.cpu().tolist()
            }

            # Store details for this batch
            prediction_log.append(batch_details)

    return prediction_log


  def evaluate(self, dataloader):
      self.model_wave.eval()
      self.model_bay.eval()
      self.model_rivermouth.eval()

      total = 0
      correct = 0

      with torch.no_grad():
          for inputs, labels in dataloader:
              inputs = inputs.to(self.device)
              labels = labels.to(self.device)

              predictions = self.predict_surfing(dataloader)

              for prediction in predictions:
                predicted_labels = prediction['overall predictions']
                # Convert predicted labels directly to a tensor and ensure it's on the right device
                predicted_labels_tensor = torch.tensor(predicted_labels).to(labels.device)

                # Debugging prints
                print(f"Predicted labels: {predicted_labels_tensor}")
                print(f"Actual labels: {labels}")

                total += labels.size(0)
                correct += (predicted_labels_tensor == labels).sum().item()

      accuracy = 100 * correct / total
      print(f'Accuracy: {accuracy:.2f}%')
      return accuracy

# Paths to the saved models
wavepattern_path = '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/model_weights_nosigmoid.pth'
bay_path = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/model_weights_nosigmoid.pth'
rivermouth_path = '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/model_weights_nosigmoid.pth'

final_model_sigmoid = FinalPredictor(wavepattern_path,bay_path,rivermouth_path)

"""### The FinalPredictor model which takes the three model and outputs as positive if one of the models outputed a positive class for the picture didn't perform well and outputed positive class for almost every picture

#Evaluation
"""

wavepattern_model = initialize_model()
bay_model = initialize_model()
# rivermouth_model = initialize_model()

# Paths to the saved models
wavepattern_path = '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/model_weights_nosigmoid.pth'
bay_path = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/model_weights_nosigmoid.pth'
# rivermouth_path = '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/model_weights_nosigmoid.pth'

# Load the models
wavepattern_model = load_model_weights(wavepattern_model, wavepattern_path)
bay_model = load_model_weights(bay_model, bay_path)
# rivermouth_model = load_model_weights(rivermouth_model, rivermouth_path)

# Move Models to GPU

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

wavepattern_model = wavepattern_model.to(device)
bay_model = bay_model.to(device)
# rivermouth_model = rivermouth_model.to(device)

transforms_test = transforms.Compose([
    transforms.Resize((224, 224)),   #must same as here
     transforms.CenterCrop((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

rivermouth_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model/val + test'
rivermouth_test_val_dataset = datasets.ImageFolder(rivermouth_test_val_dir, transforms_test)
rivermouth_val_test_dataloader = torch.utils.data.DataLoader(rivermouth_test_val_dataset, batch_size=12, shuffle=False, num_workers=0)


bay_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val + test'
bay_test_val_dataset = datasets.ImageFolder(bay_test_val_dir, transforms_test)
bay_val_test_dataloader = torch.utils.data.DataLoader(bay_test_val_dataset, batch_size=12, shuffle=False, num_workers=0)

wavepattern_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model/val + test'
wavepattern_test_val_dataset = datasets.ImageFolder(wavepattern_test_val_dir, transforms_test)
wavepattern_val_test_dataloader = torch.utils.data.DataLoader(wavepattern_test_val_dataset, batch_size=12, shuffle=False, num_workers=0)

alltogether_test_val_dir = r'/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model/val + test'
alltogether_test_val_dataset = datasets.ImageFolder(alltogether_test_val_dir, transforms_test)
alltogether_val_test_dataloader = torch.utils.data.DataLoader(alltogether_test_val_dataset, batch_size=12, shuffle=False, num_workers=0)

"""##Rivermouth"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix fo Rivermouth Model')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Rivermouth', 1: 'Rivermouth'}
evaluate_model(rivermouth_model, rivermouth_val_test_dataloader, device, class_names)

"""## Bay"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix for Bay Model')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Bay', 1: 'Bay'}
evaluate_model(bay_model, bay_val_test_dataloader, device, class_names)

"""## Wave Pattern"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Wave Pattern', 1: 'Wave Pattern'}
evaluate_model(wavepattern_model, wavepattern_val_test_dataloader, device, class_names)

"""## All Together Model"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch

def evaluate_model(model, dataloader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute the confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    # Print the classification report
    print(classification_report(all_labels, all_preds, target_names=list(class_names.values())))

    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
    plt.title('Confusion Matrix for All Together')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Usage
# Assuming 'model', 'val_dataloader', 'device', and 'class_names' are already defined
class_names = {0: 'Not Good for Surf', 1: 'Good for Surf'}
evaluate_model(model, alltogether_val_test_dataloader, device, class_names)